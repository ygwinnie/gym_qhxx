import streamlit as st
import gymnasium as gym
import numpy as np
import pandas as pd
import time
import os

# Set page config
st.set_page_config(page_title="LunarLander å¼ºåŒ–å­¦ä¹ å®éªŒå®¤", layout="wide")

st.title("ğŸš€ å¼ºåŒ–å­¦ä¹ å®éªŒå®¤: LunarLander")
st.markdown("""
æ¬¢è¿æ¥åˆ°æœˆçƒè¡¨é¢ï¼åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æŒ‘æˆ˜ä¸€ä¸ªæ›´éš¾çš„ä»»åŠ¡ï¼š**æ§åˆ¶ç™»æœˆèˆ±å¹³ç¨³ç€é™†**ã€‚
è¿™æ¯”å†°æ¹–æ¢é™©éš¾å¾—å¤šï¼Œå› ä¸ºçŠ¶æ€æ˜¯**è¿ç»­**çš„ï¼ˆä½ç½®ã€é€Ÿåº¦ã€è§’åº¦éƒ½æ˜¯å°æ•°ï¼Œè€Œä¸æ˜¯ç®€å•çš„æ ¼å­ï¼‰ã€‚
""")

# ==========================================
# ä¾§è¾¹æ : å‚æ•°æ§åˆ¶å°
# ==========================================
st.markdown(
    """
    <style>
        [data-testid="stSidebar"] .block-container {
            padding-top: 1rem;
            padding-bottom: 1rem;
        }
        .st-emotion-cache-16txtl3 {
            padding-top: 1rem;
        }
        /* å‚ç›´å±…ä¸­å¯¹é½ä¾§è¾¹æ çš„åˆ— */
        [data-testid="stSidebar"] [data-testid="stHorizontalBlock"] {
            align-items: center;
        }
        /* æè‡´ç´§å‡‘æ¨¡å¼ï¼šå‡å°‘ç»„ä»¶é—´çš„å‚ç›´é—´è· */
        [data-testid="stSidebar"] [data-testid="stVerticalBlock"] {
            gap: 0.2rem;
        }
        /* å¾®è°ƒæ–‡å­—å’Œæ»‘å—çš„è¾¹è· */
        [data-testid="stSidebar"] .stMarkdown {
            margin-bottom: -5px;
        }
    </style>
    """,
    unsafe_allow_html=True,
)

with st.sidebar:
    st.markdown("### ğŸ›ï¸ å®éªŒæ§åˆ¶å°")
    
    experiment_type = st.radio(
        "é€‰æ‹©å®éªŒæ¨¡å¼",
        ["1. ä¼ ç»Ÿ Q-Learning (å¤±è´¥æ¡ˆä¾‹)", "2. æ·±åº¦ Q-Network (æˆåŠŸæ¡ˆä¾‹)"],
        index=0
    )
    
    st.divider()
    
    # è¾…åŠ©å‡½æ•°ï¼šç´§å‡‘å‹æ»‘å—
    def compact_slider(label, min_v, max_v, default_v, step=None, format=None, help=None):
        col1, col2 = st.columns([0.35, 0.65])
        with col1:
            st.markdown(f"**{label}**", help=help)
        with col2:
            return st.slider("", min_v, max_v, default_v, step=step, format=format, label_visibility="collapsed")

    if experiment_type.startswith("1"):
        st.markdown("#### ğŸ”§ Q-Learning å‚æ•°")
        
        # 1. åŸºç¡€è®¾ç½®
        buckets = compact_slider("ç¦»æ•£ç²¾åº¦", 2, 10, 5, help="æŠŠæ¯ä¸ªè¿ç»­å˜é‡åˆ‡æˆå¤šå°‘ä»½ã€‚ä»½æ•°è¶Šå°‘è¶Šç²—ç³™ï¼Œä»½æ•°è¶Šå¤šçŠ¶æ€çˆ†ç‚¸ã€‚")
        episodes = compact_slider("è®­ç»ƒè½®æ•°", 100, 2000, 500, help="è®­ç»ƒæ¬¡æ•°ã€‚")
        
        # 2. ç®—æ³•å‚æ•°
        st.markdown("##### ğŸ§  ç®—æ³•å‚æ•°")
        learning_rate = compact_slider("å­¦ä¹ ç‡", 0.01, 1.0, 0.1, help="æœºå™¨äººæ¥å—æ–°çŸ¥è¯†çš„é€Ÿåº¦ã€‚")
        discount_factor = compact_slider("æŠ˜æ‰£å› å­", 0.1, 1.0, 0.99, help="æœºå™¨äººæœ‰å¤šçœ‹é‡æœªæ¥çš„å¥–åŠ±ã€‚")
        
        st.caption("æ¢ç´¢ç­–ç•¥ (Epsilon)")
        epsilon_start = compact_slider("åˆå§‹æ¢ç´¢", 0.1, 1.0, 1.0, help="åˆšå¼€å§‹æ—¶ï¼Œæœºå™¨äººæœ‰å¤šå¤§å‡ ç‡â€˜çé€›â€™ã€‚")
        epsilon_decay = compact_slider("æ¢ç´¢è¡°å‡", 0.90, 0.9999, 0.995, format="%.4f", help="æ•°å€¼è¶Šå°ï¼Œå®ƒâ€˜æ”¶å¿ƒâ€™å¾—è¶Šå¿«ã€‚")
        min_epsilon = compact_slider("æœ€å°æ¢ç´¢", 0.0, 0.5, 0.01, help="ä¿ç•™ä¸€ç‚¹ç‚¹å¥½å¥‡å¿ƒã€‚")

        # 3. é«˜çº§è®¾ç½® (Reward Shaping)
        st.markdown("##### âš™ï¸ é«˜çº§è®¾ç½®")
        crash_penalty = compact_slider("å æ¯æƒ©ç½š", -100.0, 0.0, -100.0, step=10.0, help="å æ¯æ—¶çš„é¢å¤–æƒ©ç½šåˆ†æ•°ã€‚")
        
        st.divider()
        start_q_btn = st.button("ğŸš€ å¼€å§‹ Q-Learning è®­ç»ƒ", type="primary", use_container_width=True)
        
    else:
        st.markdown("#### ğŸ› ï¸ DQN è®­ç»ƒå‚æ•°")
        st.info("è°ƒæ•´å‚æ•°ï¼Œè§‚å¯Ÿå¯¹è®­ç»ƒé€Ÿåº¦å’Œæ•ˆæœçš„å½±å“ã€‚")
        
        lr = compact_slider("å­¦ä¹ ç‡", 0.0001, 0.005, 0.0005, step=0.0001, format="%.4f", help="æœºå™¨äººä¿®æ­£é”™è¯¯çš„å¹…åº¦ã€‚å¤ªå¤§å®¹æ˜“éœ‡è¡ï¼Œå¤ªå°å­¦ä¹ å¤ªæ…¢ã€‚")
        gamma = compact_slider("æŠ˜æ‰£å› å­", 0.90, 0.99, 0.99, format="%.2f", help="æœºå™¨äººæœ‰å¤šçœ‹é‡æœªæ¥çš„å¥–åŠ±ã€‚æ¥è¿‘1è¡¨ç¤ºæœ‰è¿œè§ã€‚")
        
        st.caption("æ¢ç´¢ç­–ç•¥ (Exploration)")
        exploration_initial = compact_slider("åˆå§‹æ¢ç´¢ç‡", 0.5, 1.0, 1.0, format="%.2f", help="è®­ç»ƒå¼€å§‹æ—¶éšæœºæ¢ç´¢çš„æ¦‚ç‡ã€‚")
        exploration_final = compact_slider("æœ€ç»ˆæ¢ç´¢ç‡", 0.01, 0.2, 0.01, format="%.2f", help="æ¢ç´¢é˜¶æ®µç»“æŸåä¿ç•™çš„æ¢ç´¢æ¦‚ç‡ã€‚")
        exploration_fraction = compact_slider("æ¢ç´¢å æ¯”", 0.2, 0.8, 0.5, format="%.2f", help="è®­ç»ƒå‰æœŸç”¨äºæ¢ç´¢çš„æ—¶é—´æ¯”ä¾‹ã€‚")
        
        batch_size = compact_slider("æ‰¹æ¬¡å¤§å°", 32, 256, 64, step=32, help="æ¯æ¬¡ä»ç»éªŒæ± ä¸­å¤ä¹ å¤šå°‘æ¡ç»éªŒã€‚")
        total_timesteps = compact_slider("è®­ç»ƒæ­¥æ•°", 10000, 200000, 100000, step=10000, help="è®­ç»ƒçš„æ€»æ—¶é•¿ã€‚æ­¥æ•°è¶Šå¤šï¼Œæ•ˆæœè¶Šå¥½ã€‚ï¼ˆ100000æ­¥çº¦éœ€7-10åˆ†é’Ÿï¼‰")
        
        st.caption("ç¥ç»ç½‘ç»œç»“æ„")
        network_size = st.radio(
            "ç½‘ç»œå¤§å°",
            ["ç®€å• (128-128)", "æ ‡å‡† (256-256) æ¨è", "å¤æ‚ (512-256)"],
            index=1,
            help="ç½‘ç»œè¶Šå¤§ï¼Œå­¦ä¹ èƒ½åŠ›è¶Šå¼ºï¼Œä½†è®­ç»ƒè¶Šæ…¢ã€‚æ ‡å‡†é…ç½®é€‚åˆè¯¾å ‚ä½¿ç”¨ã€‚"
        )
        
        st.divider()
        start_train_btn = st.button("ğŸš€ å¼€å§‹è®­ç»ƒ (Start Training)", type="primary", use_container_width=True)

# ==========================================
# Helper: Discretized Wrapper
# ==========================================
class DiscretizedObservationWrapper(gym.ObservationWrapper):
    def __init__(self, env, n_buckets=5):
        super().__init__(env)
        self.n_buckets = n_buckets
        
        # LunarLander-v2 state has 8 dimensions:
        # [x, y, vx, vy, angle, v_angle, left_leg, right_leg]
        # We define bounds for each to discretize them.
        # Note: These bounds are approximate.
        self.bounds = [
            (-1.0, 1.0),   # x
            (-0.5, 1.5),   # y
            (-2.0, 2.0),   # vx
            (-2.0, 2.0),   # vy
            (-1.0, 1.0),   # angle
            (-2.0, 2.0),   # v_angle
            (0.0, 1.0),    # left_leg (boolean-ish)
            (0.0, 1.0)     # right_leg (boolean-ish)
        ]
        
    def observation(self, obs):
        discretized = []
        for i, val in enumerate(obs):
            l, h = self.bounds[i]
            # Clip value to bounds
            val = min(max(val, l), h)
            # Map to bucket index
            # p is 0..1
            p = (val - l) / (h - l)
            bucket = int(p * self.n_buckets)
            bucket = min(bucket, self.n_buckets - 1)
            discretized.append(bucket)
            
        # Convert tuple of buckets to a single integer index if possible, 
        # but for Q-table we might just use the tuple as key.
        return tuple(discretized)

# ==========================================
# State Management
# ==========================================
if 'lunar_q_table' not in st.session_state:
    st.session_state.lunar_q_table = None
if 'lunar_results' not in st.session_state:
    st.session_state.lunar_results = pd.DataFrame()
if 'lunar_training_completed' not in st.session_state:
    st.session_state.lunar_training_completed = False

# ==========================================
# Part 1: Q-Learning Implementation
# ==========================================
if experiment_type.startswith("1"):
    st.subheader("ğŸ§ª å®éªŒ 1: ä¼ ç»Ÿ Q-Learning çš„å±€é™æ€§")
    
    # --- Training Logic ---
    if start_q_btn:
        st.write("æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...")
        
        # Create environment
        try:
            env = gym.make("LunarLander-v3", render_mode=None) # No render during training
            env = DiscretizedObservationWrapper(env, n_buckets=buckets)
        except Exception as e:
            st.error(f"ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
            st.stop()
            
        # Q-Table
        q_table = {}
        
        def get_q(state, action):
            return q_table.get((state, action), 0.0)
            
        def update_q(state, action, value):
            q_table[(state, action)] = value
            
        def choose_action(state, epsilon):
            if np.random.random() < epsilon:
                return env.action_space.sample()
            else:
                q_values = [get_q(state, a) for a in range(4)]
                max_q = max(q_values)
                actions = [i for i, q in enumerate(q_values) if q == max_q]
                return np.random.choice(actions)

        # Training Loop
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        rewards_history = []
        steps_history = []
        epsilon_history = []
        
        plot_data = {
            "episode": [],
            "success_rate": [],
            "avg_steps": [],
            "avg_reward": [],
            "epsilon": []
        }
        
        epsilon = epsilon_start
        alpha = learning_rate
        gamma = discount_factor
        
        start_time = time.time()
        
        for ep in range(episodes):
            state, _ = env.reset()
            total_reward = 0
            done = False
            steps = 0
            
            while not done and steps < 500:
                action = choose_action(state, epsilon)
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                # Reward Shaping
                custom_reward = reward
                if terminated and reward == -100:
                    custom_reward = crash_penalty
                
                # Update Q
                old_q = get_q(state, action)
                next_max_q = max([get_q(next_state, a) for a in range(4)])
                new_q = old_q + alpha * (custom_reward + gamma * next_max_q - old_q)
                update_q(state, action, new_q)
                
                state = next_state
                total_reward += reward
                steps += 1
            
            epsilon = max(min_epsilon, epsilon * epsilon_decay)
            rewards_history.append(total_reward)
            steps_history.append(steps)
            epsilon_history.append(epsilon)
            
            if (ep + 1) % 10 == 0:
                recent_rewards = rewards_history[-10:]
                avg_reward = np.mean(recent_rewards)
                avg_steps = np.mean(steps_history[-10:])
                success_rate = sum(r > 200 for r in recent_rewards) / len(recent_rewards) * 100
                
                plot_data["episode"].append(ep + 1)
                plot_data["success_rate"].append(success_rate)
                plot_data["avg_steps"].append(avg_steps)
                plot_data["avg_reward"].append(avg_reward)
                plot_data["epsilon"].append(epsilon)
                
                progress_bar.progress((ep + 1) / episodes)
                status_text.text(f"Training... Episode {ep+1}/{episodes} | Avg Reward: {avg_reward:.1f}")
                
        env.close()
        st.success(f"âœ… è®­ç»ƒå®Œæˆï¼è€—æ—¶: {time.time() - start_time:.2f} ç§’")
        
        # Save to session state
        st.session_state.lunar_q_table = q_table
        st.session_state.lunar_results = pd.DataFrame(plot_data).set_index("episode")
        st.session_state.lunar_training_completed = True

    # --- Results & Testing Logic ---
    if st.session_state.lunar_training_completed:
        results_df = st.session_state.lunar_results
        q_table = st.session_state.lunar_q_table
        
        # 1. Charts
        st.markdown("### ğŸ“ˆ å­¦ä¹ è¿‡ç¨‹åˆ†æ")
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ† èƒœç‡", "ğŸ‘£ å¹³å‡æ­¥æ•°", "ğŸ’° å¥–åŠ±åˆ†æ•°", "ğŸ² æ¢ç´¢ç‡"])
        
        with tab1:
            st.line_chart(results_df["success_rate"])
            st.caption("èƒœç‡ (åˆ†æ•° > 200) è¶Šé«˜ï¼Œè¯´æ˜ç€é™†è¶ŠæˆåŠŸã€‚")
        with tab2:
            st.line_chart(results_df["avg_steps"])
            st.caption("æ­¥æ•°è¶Šå°‘ï¼Œè¯´æ˜ç€é™†è¶Šå¿«ã€‚")
        with tab3:
            st.line_chart(results_df["avg_reward"])
            st.caption("åˆ†æ•°è¶Šé«˜ï¼Œè¯´æ˜ç€é™†è´¨é‡è¶Šå¥½ã€‚")
        with tab4:
            st.line_chart(results_df["epsilon"])
            st.caption("æ¢ç´¢ç‡é€æ¸é™ä½ï¼Œæœºå™¨äººè¶Šæ¥è¶Šä¾èµ–ç»éªŒã€‚")
            
        st.info("""
        **ğŸ¤” ä¸ºä»€ä¹ˆèƒœç‡è¿™ä¹ˆä½/ä¸ç¨³å®šï¼Ÿ**
        
        è¿™æ­£æ˜¯**ä¼ ç»Ÿ Q-Learning** åœ¨å¤æ‚è¿ç»­ç¯å¢ƒä¸­çš„å…¸å‹è¡¨ç°ï¼ˆå¤±è´¥æ¡ˆä¾‹ï¼‰ï¼š
        1.  **çŠ¶æ€ç©ºé—´çˆ†ç‚¸**ï¼šå³ä½¿æˆ‘ä»¬å°†æ¯ä¸ªç»´åº¦åªåˆ‡æˆ 5 ä»½ï¼Œæ€»çŠ¶æ€æ•°ä¹Ÿé«˜è¾¾ $5^8 = 390,625$ ä¸ªï¼çŸ­çŸ­å‡ ç™¾è½®è®­ç»ƒæ ¹æœ¬æ— æ³•å¡«æ»¡ Q è¡¨ï¼Œå¤§éƒ¨åˆ†çŠ¶æ€æœºå™¨äººä»æœªè§è¿‡ã€‚
        2.  **ç²¾åº¦ä¸¢å¤±**ï¼šä¸ºäº†ä½¿ç”¨ Q è¡¨ï¼Œæˆ‘ä»¬å°†è¿ç»­çš„ä½ç½®å’Œé€Ÿåº¦â€œæ¨¡ç³ŠåŒ–â€äº†ï¼ˆç¦»æ•£åŒ–ï¼‰ã€‚è¿™å¯¼è‡´æœºå™¨äººæ— æ³•æ„ŸçŸ¥ç»†å¾®çš„å˜åŒ–ï¼Œå°±åƒæˆ´ç€åšæ‰‹å¥—ç©¿é’ˆå¼•çº¿ï¼Œå¾ˆéš¾ç²¾å‡†æ§åˆ¶ã€‚
        3.  **è¿æ°”æˆåˆ†**ï¼šå¶å°”å‡ºç°çš„æˆåŠŸï¼ˆæ³¢å³°ï¼‰å¯èƒ½åªæ˜¯å› ä¸ºåˆå§‹ä½ç½®è¾ƒå¥½ï¼Œæˆ–è€…æœºå™¨äººâ€œè’™â€å¯¹äº†ä¸€æ¡è·¯ï¼Œä½†å®ƒå¹¶æ²¡æœ‰çœŸæ­£å­¦ä¼šé€šç”¨çš„é£è¡ŒæŠ€å·§ã€‚
        
        ğŸ‘‰ **è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ·±åº¦å¼ºåŒ–å­¦ä¹  (DQN)ï¼** è¯·å°è¯•åˆ‡æ¢åˆ°â€œå®éªŒ 2â€çœ‹çœ‹åŒºåˆ«ã€‚
        """)
            
        st.divider()
        
        # 2. Test Section
        st.markdown("### ğŸ¥ å®æˆ˜æ¼”ç¤º (10è½®)")
        st.info("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ï¼ŒæŸ¥çœ‹å½“å‰æ¨¡å‹çš„å®é™…è¡¨ç°ã€‚")
        run_test_btn = st.button("å¼€å§‹æµ‹è¯• (Run Test)", use_container_width=True)
        
        if run_test_btn:
            col_anim, col_stats = st.columns([2, 1])
            with col_anim:
                frame_placeholder = st.empty()
            with col_stats:
                st.markdown("#### æµ‹è¯•ç»Ÿè®¡")
                metric_success_rate = st.empty()
                metric_avg_reward = st.empty()
                metric_steps = st.empty()
            
            # Re-create environment for rendering
            try:
                env = gym.make("LunarLander-v3", render_mode="rgb_array")
                env = DiscretizedObservationWrapper(env, n_buckets=buckets)
            except:
                st.error("ç¯å¢ƒåˆ›å»ºå¤±è´¥")
                st.stop()
                
            success_count = 0
            total_steps = 0
            total_test_reward = 0
            
            # Helper to get Q
            def get_q_test(state, action):
                return q_table.get((state, action), 0.0)
            
            for i in range(10):
                state, _ = env.reset()
                done = False
                steps = 0
                episode_reward = 0
                
                while not done and steps < 500:
                    frame = env.render()
                    frame_placeholder.image(frame, caption=f"Test Episode {i+1}/10 | Step {steps}", use_container_width=True)
                    
                    # Greedy action
                    q_values = [get_q_test(state, a) for a in range(4)]
                    action = np.argmax(q_values)
                    
                    state, reward, terminated, truncated, _ = env.step(action)
                    done = terminated or truncated
                    steps += 1
                    episode_reward += reward
                    time.sleep(0.01) # Slow down animation
                
                if reward == 100 or reward > 200: # Approximate success check
                    success_count += 1
                total_steps += steps
                total_test_reward += episode_reward
                
                metric_success_rate.metric("æˆåŠŸç‡ (Success Rate)", f"{success_count/(i+1)*100:.0f}% ({success_count}/{i+1})")
                metric_avg_reward.metric("å¹³å‡å¥–åŠ± (Avg Reward)", f"{total_test_reward/(i+1):.1f}")
                metric_steps.metric("å¹³å‡æ­¥æ•° (Avg Steps)", f"{total_steps/(i+1):.1f}")
                
            env.close()
            
            if success_count >= 8: st.success("ğŸ† è¡¨ç°ä¼˜ç§€ï¼")
            elif success_count >= 5: st.warning("ğŸ˜ è¡¨ç°ä¸€èˆ¬")
            else: st.error("ğŸ’€ è¿˜éœ€è¦åŠªåŠ›")

    else:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è°ƒæ•´å‚æ•°ï¼Œç„¶åç‚¹å‡» 'å¼€å§‹è®­ç»ƒ' æŒ‰é’®ã€‚")

# ==========================================
# Part 2: DQN Implementation
# ==========================================
elif experiment_type.startswith("2"):
    st.subheader("ğŸ§  å®éªŒ 2: æ·±åº¦å¼ºåŒ–å­¦ä¹  (DQN) çš„å¨åŠ›")
    
    st.markdown("""
    **DQN (Deep Q-Network)** ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥ç›´æ¥å¤„ç†è¿ç»­çš„çŠ¶æ€è¾“å…¥ï¼Œä¸å†éœ€è¦äººå·¥è¿›è¡Œç¦»æ•£åŒ–ã€‚
    
    åœ¨è¿™ä¸ªå®éªŒå®¤ä¸­ï¼Œä½ å°†äº²æ‰‹è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œï¼
    """)
    
    st.divider()

    # --- Main Area: Training Dashboard ---
    st.markdown("### ğŸ‹ï¸â€â™‚ï¸ DQN è®­ç»ƒå®éªŒå®¤")
    
    # Create persistent placeholders for charts and status
    status_text = st.empty()
    progress_bar = st.empty()
    
    # Three-column chart layout
    chart_col1, chart_col2, chart_col3 = st.columns(3)
    with chart_col1:
        st.markdown("**ğŸ“ˆ å¹³å‡å¥–åŠ±**")
        chart_reward = st.empty()
    with chart_col2:
        st.markdown("**âœ… æˆåŠŸç‡**")
        chart_success = st.empty()
    with chart_col3:
        st.markdown("**ğŸ” æ¢ç´¢ç‡**")
        chart_exploration = st.empty()
    
    # Display saved training charts if exists
    if 'dqn_training_data' in st.session_state and st.session_state.dqn_training_data is not None:
        data = st.session_state.dqn_training_data
        
        df_reward = pd.DataFrame({
            "steps": data['steps'], 
            "å½“å‰å¥–åŠ±": data['episode_reward'],
            "å¹³å‡å¥–åŠ±": data['avg_reward']
        }).set_index("steps")
        chart_reward.line_chart(df_reward, height=200, color=["#1f77b4", "#cccccc"])
        
        df_success = pd.DataFrame({"steps": data['steps'], "success_rate": data['success_rate']}).set_index("steps")
        chart_success.line_chart(df_success, height=200)
        
        df_exploration = pd.DataFrame({"steps": data['steps'], "exploration_rate": data['exploration_rate']}).set_index("steps")
        chart_exploration.line_chart(df_exploration, height=200)
        
        status_text.success(f"âœ… è®­ç»ƒå®Œæˆï¼(å…± {data['steps'][-1]} æ­¥)")

    # --- Training Logic ---
    if start_train_btn:
        try:
            from stable_baselines3 import DQN
            from stable_baselines3.common.callbacks import BaseCallback
        except ImportError:
            st.error("è¯·å…ˆå®‰è£… stable-baselines3: `pip install stable-baselines3 shimmy gymnasium[box2d]`")
            st.stop()

        # Custom Callback for Streamlit
        class StreamlitCallback(BaseCallback):
            def __init__(self, status_text, progress_bar, chart_reward, chart_success, chart_exploration, verbose=0):
                super().__init__(verbose)
                self.episode_rewards = []  # Individual episode rewards
                self.avg_rewards = []      # Moving average rewards
                self.success_rates = []
                self.exploration_rates = []
                self.timesteps = []
                
                # Smoothing for success rate
                self.smoothed_success_rate = 0.0
                self.alpha = 0.1  # Smoothing factor (0.1 = smooth, 0.9 = responsive)
                
                # UI placeholders (passed from outside)
                self.status_text = status_text
                self.progress_bar = progress_bar
                self.chart_reward = chart_reward
                self.chart_success = chart_success
                self.chart_exploration = chart_exploration
                    
            def _on_step(self) -> bool:
                # Update progress
                percent = min(self.num_timesteps / total_timesteps, 1.0)
                with self.progress_bar:
                    st.progress(percent)
                self.status_text.text(f"æ­£åœ¨è®­ç»ƒç¥ç»ç½‘ç»œ... è¿›åº¦: {self.num_timesteps}/{total_timesteps} æ­¥")
                
                # Capture metrics every 200 steps to reduce overhead
                if self.num_timesteps % 200 == 0:
                    # Track both episode reward and average reward
                    if len(self.model.ep_info_buffer) > 0:
                        # Get the most recent episode reward (latest completed episode)
                        latest_ep_reward = self.model.ep_info_buffer[-1]['r']
                        self.episode_rewards.append(latest_ep_reward)
                        
                        # Calculate moving average
                        avg_reward = np.mean([ep_info['r'] for ep_info in self.model.ep_info_buffer])
                        self.avg_rewards.append(avg_reward)
                        
                        # Success rate: episodes with reward > 0 (successful landing)
                        # Only calculate if buffer has enough data
                        if len(self.model.ep_info_buffer) >= 5:
                            success_count = sum(1 for ep in self.model.ep_info_buffer if ep['r'] > 0)
                            raw_success_rate = (success_count / len(self.model.ep_info_buffer)) * 100
                            
                            # Apply exponential moving average for smoothing
                            self.smoothed_success_rate = self.alpha * raw_success_rate + (1 - self.alpha) * self.smoothed_success_rate
                            self.success_rates.append(self.smoothed_success_rate)
                        else:
                            # Not enough data yet
                            self.success_rates.append(0)
                    else:
                        self.episode_rewards.append(0)
                        self.avg_rewards.append(0)
                        self.success_rates.append(0)
                    
                    # Exploration rate (linear decay)
                    exploration_rate = self.model.exploration_rate
                    self.exploration_rates.append(exploration_rate)
                    
                    self.timesteps.append(self.num_timesteps)
                    
                    # Update reward chart with both lines
                    if len(self.episode_rewards) > 1:
                        df_reward = pd.DataFrame({
                            "steps": self.timesteps, 
                            "å½“å‰å¥–åŠ±": self.episode_rewards,
                            "å¹³å‡å¥–åŠ±": self.avg_rewards
                        }).set_index("steps")
                        self.chart_reward.line_chart(df_reward, height=200, color=["#1f77b4", "#cccccc"])
                        
                        df_success = pd.DataFrame({"steps": self.timesteps, "success_rate": self.success_rates}).set_index("steps")
                        self.chart_success.line_chart(df_success, height=200)
                        
                        df_exploration = pd.DataFrame({"steps": self.timesteps, "exploration_rate": self.exploration_rates}).set_index("steps")
                        self.chart_exploration.line_chart(df_exploration, height=200)
                        
                return True

        # Init Environment & Model
        env = gym.make("LunarLander-v3", render_mode=None)
        
        # Map network size selection to architecture (research-backed sizes)
        net_arch_map = {
            "ç®€å• (128-128)": [128, 128],
            "æ ‡å‡† (256-256) æ¨è": [256, 256],  # Proven successful for LunarLander
            "å¤æ‚ (512-256)": [512, 256]
        }
        policy_kwargs = dict(net_arch=net_arch_map[network_size])
        
        model = DQN(
            "MlpPolicy", 
            env, 
            policy_kwargs=policy_kwargs,  # Custom network architecture
            learning_rate=lr,
            gamma=gamma,
            exploration_initial_eps=exploration_initial,
            exploration_final_eps=exploration_final,
            exploration_fraction=exploration_fraction,
            batch_size=batch_size,
            buffer_size=100000,  # Increased from 50000 for better retention
            learning_starts=1000,  # Start learning after collecting some experiences
            train_freq=4,  # Train every 4 steps
            gradient_steps=1,  # One gradient step per training
            target_update_interval=1000, # Update target network every 1000 steps
            verbose=0,
            device="auto"
        )
        
        status_text.write("æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒå’Œç¥ç»ç½‘ç»œ...")
        callback = StreamlitCallback(status_text, progress_bar, chart_reward, chart_success, chart_exploration)
        
        # Start Training
        model.learn(total_timesteps=total_timesteps, callback=callback)
        
        status_text.success(f"âœ… è®­ç»ƒå®Œæˆï¼(å…± {total_timesteps} æ­¥)")
        progress_bar.empty()  # Clear progress bar
        
        # Save training data to session state
        st.session_state.dqn_training_data = {
            "steps": callback.timesteps,
            "episode_reward": callback.episode_rewards,
            "avg_reward": callback.avg_rewards,
            "success_rate": callback.success_rates,
            "exploration_rate": callback.exploration_rates
        }
        
        # Save to session state
        st.session_state.dqn_model = model
        st.session_state.dqn_source = "student"
        
        # Force rerun to show test section
        time.sleep(1)
        st.rerun()

    # --- Test Section (Only visible if model exists) ---
    if st.session_state.get('dqn_model') is not None and st.session_state.get('dqn_source') == 'student':
        st.divider()
        st.markdown("### ğŸ¥ æˆæœéªŒæ”¶")
        st.info('è®­ç»ƒç»“æŸäº†ï¼è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ª"æ–°æ‰‹"æœºå™¨äººçš„è¡¨ç°å¦‚ä½•ã€‚')
        
        run_test_btn = st.button("â–¶ï¸ è¿è¡Œæµ‹è¯• (10è½®)", use_container_width=True)
        
        if run_test_btn:
            model = st.session_state.dqn_model
            env = gym.make("LunarLander-v3", render_mode="rgb_array")
            
            col_anim, col_stats = st.columns([2, 1])
            with col_anim:
                frame_placeholder = st.empty()
            with col_stats:
                st.markdown("#### ğŸ“Š å®æ—¶ç»Ÿè®¡")
                metric_success = st.empty()
                metric_reward = st.empty()
                metric_steps = st.empty()
            
            # Run 10 episodes
            success_count = 0
            total_reward = 0
            total_steps = 0
            
            for episode in range(10):
                obs, _ = env.reset()
                done = False
                steps = 0
                ep_reward = 0
                
                while not done and steps < 500:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, _ = env.step(action)
                    done = terminated or truncated
                    ep_reward += reward
                    steps += 1
                    
                    if steps % 2 == 0:
                        frame = env.render()
                        # Show timeout warning if approaching limit
                        status = f"Episode {episode+1}/10 | Step {steps}"
                        if steps >= 400:
                            status += " âš ï¸ æ¥è¿‘è¶…æ—¶"
                        frame_placeholder.image(frame, caption=status, use_container_width=True)
                
                # Update stats
                if ep_reward > 200:
                    success_count += 1
                total_reward += ep_reward
                total_steps += steps
                
                # Update metrics
                current_success_rate = (success_count / (episode + 1)) * 100
                current_avg_reward = total_reward / (episode + 1)
                current_avg_steps = total_steps / (episode + 1)
                
                metric_success.metric("æˆåŠŸç‡", f"{current_success_rate:.0f}% ({success_count}/{episode+1})")
                metric_reward.metric("å¹³å‡å¥–åŠ±", f"{current_avg_reward:.1f}")
                metric_steps.metric("å¹³å‡æ­¥æ•°", f"{current_avg_steps:.1f}")
            
            env.close()
            
            # Final evaluation
            final_success_rate = (success_count / 10) * 100
            if final_success_rate >= 80:
                st.balloons()
                st.success("ğŸ‰ è¡¨ç°ä¼˜ç§€ï¼è¿™ä¸ªæ¨¡å‹å·²ç»å­¦ä¼šäº†ä¸å°‘æŠ€å·§ã€‚")
            elif final_success_rate >= 50:
                st.info("ğŸ˜ è¡¨ç°ä¸€èˆ¬ã€‚å¯ä»¥å°è¯•è°ƒæ•´å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ­¥æ•°ã€‚")
            else:
                st.error("ğŸ’¥ è¡¨ç°è¾ƒå·®ã€‚å»ºè®®å¢åŠ è®­ç»ƒæ­¥æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡ã€‚")
