# 📘 FrozenLake 互动教学指南 (User Manual)

本指南旨在帮助教师和学生更好地使用 **FrozenLake 强化学习实验室**。通过本工具，您可以直观地探索强化学习（Reinforcement Learning）是如何工作的。

---

## 1. 界面功能说明

### 1.1 左侧：控制台 (Control Panel)
这里是“指挥中心”，您可以调整训练机器人的各种参数。
*   **基础设置**: 控制训练的轮数和环境难度（是否打滑）。
*   **算法参数**: 调整 Q-Learning 算法的核心逻辑（学习速度、远见、好奇心）。
*   **高级设置**: 通过“奖励重塑”来改变机器人的价值观（比如让它更怕掉坑）。

### 1.2 右侧：实验结果 (Results)
*   **📈 学习过程分析**:
    *   **胜率**: 机器人拿到礼物的成功率。
    *   **平均步数**: 机器人走到终点（或掉坑）平均走了多少步。
    *   **奖励分数**: 机器人的“得分”情况。
    *   **探索率**: 机器人“好奇心”的变化曲线。
*   **🗺️ 策略地图**: 
    *   展示了机器人“脑子”里的地图。
    *   箭头指向哪里，就代表机器人在那个格子觉得往哪里走最好。
*   **🎥 实战演示**: 
    *   点击“开始测试”按钮，观看训练好的机器人现场表演 10 次。
*   **🧠 Q表数值**: 
    *   展示 Q-Table 的原始数据（颜色越深代表数值越大，机器人越倾向于该动作）。

---

## 2. 参数详解 (Parameter Guide)

为了方便教学，以下是各个参数的通俗解释：

| 参数名 | 英文名 | 含义解释 | 教学隐喻 |
| :--- | :--- | :--- | :--- |
| **训练轮数** | Episodes | 机器人练习的总次数。 | 就像学生做练习题，做得越多通常越熟练。 |
| **冰面打滑** | Slippery | 是否开启环境的随机性。 | 就像在溜冰场上，你想往左走，但可能会滑到前边去。 |
| **学习率** | Learning Rate (Alpha) | 机器人接受新知识的速度。 | **高**: 喜新厌旧（容易不稳定）。<br>**低**: 固步自封（学得慢）。 |
| **折扣因子** | Discount Factor (Gamma) | 机器人有多看重未来的奖励。 | **0**: 短视（只看眼前）。<br>**1**: 远见（重视长期利益）。 |
| **初始探索** | Epsilon Start | 刚开始时的“瞎逛”概率。 | 就像刚到一个新城市，你会到处乱走来熟悉地形。 |
| **探索衰减** | Epsilon Decay | “瞎逛”概率减少的速度。 | 随着时间推移，你越来越熟悉路了，就不再乱走了。 |
| **掉坑惩罚** | Hole Penalty | 掉进冰窟窿扣多少分。 | 如果惩罚很重（比如 -10），机器人会变得非常谨慎（甚至不敢动）。 |
| **步数消耗** | Step Cost | 每走一步扣多少分。 | 如果每一步都扣分，机器人会想办法“抄近道”尽快完成。 |

### 2.2 LunarLander 参数 (进阶)

| 参数名 | 英文名 | 含义解释 | 教学隐喻 |
| :--- | :--- | :--- | :--- |
| **离散精度** | Discretization Buckets | 把连续的状态切成多少份。 | 就像把高清照片变成马赛克。格子越少越模糊，格子越多越清晰（但也越难学）。 |
| **坠毁惩罚** | Crash Penalty | 飞船坠毁时扣多少分。 | 就像飞行员考试，坠机是绝对不能接受的严重错误。 |
| **DQN 模型** | Deep Q-Network | 使用深度神经网络来代替 Q 表。 | 就像从“死记硬背”（查表）进化到了“融会贯通”（神经网络），能处理更复杂的情况。 |


---

## 3. DQN 训练实验室 (DQN Training Lab)

### 3.1 DQN 可调参数详解

| 参数名 | 范围 | 默认值 | 含义解释 | 课堂建议 |
|:---|:---|:---|:---|:---|
| **学习率** | 0.0001~0.005 | 0.001 | 神经网络调整速度 | 太大易震荡，太小学习慢。建议0.0005~0.001 |
| **折扣因子** | 0.90~0.99 | 0.99 | 对未来奖励的重视程度 | 接近1表示有远见，通常保持0.99 |
| **初始探索率** | 0.5~1.0 | 1.0 | 训练开始时随机探索概率 | 建议保持1.0，充分探索 |
| **最终探索率** | 0.01~0.2 | 0.05 | 探索结束后保留的探索 | **关键参数**！太低会导致后期遗忘，建议0.10~0.15 |
| **探索占比** | 0.2~0.5 | 0.3 | 探索阶段占总训练的比例 | 0.3~0.5较合适，给予充分探索时间 |
| **批次大小** | 32~256 | 128 | 每次从经验池复习的样本数 | 128是平衡值，不建议调整 |
| **训练步数** | 10k~200k | 50,000 | 训练总时长 | 50k步约5-6分钟，200k步约15-20分钟 |

### 3.2 训练曲线解读

**平均奖励图**（双曲线）：
- 蓝色线（当前奖励）：每个episode的真实得分，会有较大波动
- 灰色线（平均奖励）：滚动平均值，显示整体趋势
- **正常模式**：整体上升，波动逐渐减小
- **异常模式**：先升后降，说明探索不足或参数不当

**成功率图**：
- 成功标准：episode奖励 ≥ 0（没有坠毁）
- 使用指数移动平均平滑，避免锯齿
- **目标**：40-60%成功率为良好水平
- **警告**：如果探索结束后急剧下降，需提高最终探索率

**探索率图**：
- 前 `探索占比 × 总步数` 内线性下降
- 之后保持在最终探索率
- 配合奖励曲线观察：探索率下降时奖励应稳定上升

### 3.3 常见训练问题

#### ⚠️ 问题1：成功率先升后降（灾难性遗忘）
**症状**：探索期成功率20-30%，探索结束后降到0-10%

**原因**：
- 最终探索率过低（<0.05）
- 后期只利用不探索，陷入局部最优
- 新失败经验覆盖旧成功经验

**解决方案**：
- 提高最终探索率到0.10~0.15
- 降低学习率到0.0005~0.0008
- 延长探索占比到0.4~0.5

#### ⚠️ 问题2：训练不稳定，波动剧烈
**症状**：奖励曲线剧烈抖动，难以收敛

**原因**：
- 学习率过高
- 批次大小过小

**解决方案**：
- 降低学习率到0.0005
- 批次大小保持128或更高

#### ⚠️ 问题3：Episode提前结束（平均步数<500）
**症状**：测试时火箭还没落地就结束

**原因**：
- **坠毁**（最常见）：火箭撞击地面，平均步数200-400
- **超时**（少见）：超过500步还未降落

**解决方案**：
- 继续优化训练参数
- 增加训练步数到100k-200k
- 确保最终探索率≥0.10

### 3.4 推荐训练配置

**快速验证版（约3-4分钟）**：
```
学习率：0.0012
最终探索率：0.15
探索占比：0.3
训练步数：40,000
预期成功率：20-30%
```

**课堂推荐版（约5-7分钟）**：
```
学习率：0.0008
最终探索率：0.12
探索占比：0.4
训练步数：80,000
预期成功率：40-50%
```

**最佳效果版（约15-20分钟）**：
```
学习率：0.0005
最终探索率：0.10
探索占比：0.5
训练步数：200,000
预期成功率：50-70%
```

---

## 4. 常见问题 (FAQ)

### Q1: 为什么训练曲线有时候波动很大？
**A**: 强化学习本身就是一个不断“试错”的过程。特别是在开启“冰面打滑”模式后，即使策略是对的，机器人也可能因为运气不好而滑进坑里，导致胜率波动。

### Q2: 为什么机器人有时候会在原地打转？
**A**: 这可能是因为：
1.  **步数消耗 (Step Cost)** 设置为 0，机器人觉得“多走几步也没关系”。
2.  **掉坑惩罚** 太大，导致机器人觉得“只要不掉坑就是胜利”，所以宁愿在安全的地方转圈也不敢冒险去终点。
    *   *教学建议*: 试着增加一点“步数消耗”（比如 -0.01），逼迫它动起来。

### Q3: 为什么 Q 表里有很多 0？
**A**: 
1.  有些格子（比如洞或者终点）是终止状态，没有后续动作。
2.  有些格子机器人从来没有走到过（探索不够），所以它不知道那里有什么。

---

## 4. 课堂活动建议

1.  **“性格测试”**: 让学生调整参数，训练出不同性格的机器人：
    *   **急躁型**: 步数消耗设得很高。
    *   **胆小型**: 掉坑惩罚设得很高。
    *   **短视型**: 折扣因子设为 0。
2.  **“运气与实力”**: 对比开启和关闭“冰面打滑”对训练结果的影响，讨论“环境不确定性”对决策的挑战。
